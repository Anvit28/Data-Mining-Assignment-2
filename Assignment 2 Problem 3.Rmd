---
title: "Data Mining Assignment 2"
author: ""
date: ""
output:
  md_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r installing packages 1, echo=FALSE, message=FALSE, warning=FALSE}
install.packages("tidyverse",dependencies = TRUE)
```
# Problem 3 
Dataset: hotels_dev.csv and hotels_val.csv
```{r importing libraries 2, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
```
```{r Reading Dataset 3, echo=FALSE, warning=FALSE}
hotels_dev = read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/hotels_dev.csv")
hotels_dev2=hotels_dev
```
First, we will convert the data into numeric form by doing label encodng.  
```{r label encoding 4, echo=FALSE, message=FALSE, warning=FALSE}
hotels_dev$market_segment <- as.numeric(factor(hotels_dev$market_segment))
hotels_dev$customer_type <- as.numeric(factor(hotels_dev$customer_type))
hotels_dev$hotel <- as.numeric(factor(hotels_dev$hotel))
hotels_dev$meal <- as.numeric(factor(hotels_dev$meal))
hotels_dev$distribution_channel <- as.numeric(factor(hotels_dev$distribution_channel))
hotels_dev$reserved_room_type <- as.numeric(factor(hotels_dev$reserved_room_type))
hotels_dev$assigned_room_type <- as.numeric(factor(hotels_dev$assigned_room_type))
hotels_dev$deposit_type <- as.numeric(factor(hotels_dev$deposit_type))
hotels_dev$required_car_parking_spaces <- as.numeric(factor(hotels_dev$required_car_parking_spaces))
```
Next, we check the number of data points in each class (i.e., children=0 and children=1) in the data.  
```{r summarizing data 5, echo=FALSE, message=FALSE, warning=FALSE}
table(hotels_dev$children)
```
We can see that the data is imbalanced. It comprises of more than 90% data points that have zero children.  
To resolve this problem, I formed training set with 6000 data points with zero children, and 3000 data points with one children. So the training set has two-thirds of the data points with zero children, and one-third of the data points with at least one children. The test data is comprised of 2000 data points with zero children, and 650 data points with one children.  
```{r forming training and test data 6, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(12345) 
hotels_dev=sample_n(hotels_dev,nrow(hotels_dev))
class_1=hotels_dev[hotels_dev$children==1,]
train_1=class_1[0:3000,]
test_1=class_1[3001:3650,]
class_0=hotels_dev[hotels_dev$children==0,]
train_0=class_0[0:6000,]
test_0=class_0[6001:8000,]
training_data=rbind(train_0,train_1)
set.seed(12345)
training_data=sample_n(training_data,nrow(training_data))
test_data=rbind(test_0,test_1)
set.seed(12345)
test_data=sample_n(test_data,nrow(test_data))
```
Now we start with creating the models.  
To evaluate the models, we calculate the out-sample confusion matrix, accuracy, precision, recall and F1 Score, which are given as:-  
Accuracy = (True Positive+True Negative)/(True Positive+False Positive+True Negative+False Negative)  
Precision = True Positive / (True Positive+False Positive)  
Recall = True Positive / (True Positive+False Negative)  
F1 Score = (2 * Precision * Recall)/(Precision + Recall)  
The baseline 1 model is the logistic regression model with market_segment, adults, customer_type, and is_repeated_guest variables as features.  
```{r baseline 1 model, echo=FALSE, message=FALSE, warning=FALSE}
baseline1=glm(children ~ market_segment + adults + customer_type + is_repeated_guest, data=training_data,family = 'binomial')
```
Let's look at the in-sample and out-sample performance of baseline 1 model.  
```{r in-sample performance of baseline 1 model, echo=FALSE, message=FALSE, warning=FALSE}
predicted_children_probabilities=baseline1 %>% predict(training_data,type="response")
predicted_children = ifelse(predicted_children_probabilities>0.5,1,0)
confusion_in=table(y=training_data$children,yhat=predicted_children)
print("The in-sample confusion matrix of baseline 1 model is:-")
print(confusion_in)
print(paste0("The in-sample accuracy of baseline 1 model is: ", sum(diag(confusion_in))/sum(confusion_in)))
```
```{r out-of-sample performance of baseline 1 model, echo=FALSE, message=FALSE, warning=FALSE}
predicted_children_probabilities=baseline1 %>% predict(test_data,type="response")
predicted_children = ifelse(predicted_children_probabilities>0.5,1,0)
print("The out-of-sample confusion matrix of baseline 1 model is:-")
confusion_out=table(y=test_data$children,yhat=predicted_children)
print(confusion_out)
baseline1_accuracy=sum(diag(confusion_out))/sum(confusion_out)
print(paste0("The out-of-sample accuracy of baseline 1 model is: ",baseline1_accuracy))
baseline1_precision=confusion_out[4]/(confusion_out[3]+confusion_out[4])
baseline1_recall=confusion_out[4]/(confusion_out[2]+confusion_out[4])
baseline1_f1score=2*baseline1_precision*baseline1_recall/(baseline1_precision+baseline1_recall)
print(paste0("The out-of-sample precision of baseline 1 model is: ",baseline1_precision))
print(paste0("The out-of-sample recall of baseline 1 model is: ",baseline1_recall))
print(paste0("The out-of-sample F1 Score of baseline 1 model is: ",baseline1_f1score))
```
We can see from the above result that the baseline 1 model has a decent accuracy but very low precision, recall and F1 Score. This is because this model is unable to accurately predict the data points with at least one children.  
  
The baseline 2 model is the logistic regression model with that uses all the possible predictors except the arrival_date variable. Let's look at the in-sample and out-sample performance of baseline 2 model.  
```{r forming training and test data for baseline 2 model, echo=FALSE, message=FALSE, warning=FALSE}
training_data2=subset(training_data,select=-c(arrival_date))
test_data2=subset(test_data,select=-c(arrival_date))
```
```{r forming baseline 2 model, echo=FALSE, message=FALSE, warning=FALSE}
baseline2=glm(children ~ ., data=training_data2,family = 'binomial')
```
```{r in-sample performance of baseline 2 model, echo=FALSE, message=FALSE, warning=FALSE}
predicted_children_probabilities=baseline2 %>% predict(training_data2,type="response")
predicted_children = ifelse(predicted_children_probabilities>0.5,1,0)
confusion_in=table(y=training_data2$children,yhat=predicted_children)
print("The in-sample confusion matrix of baseline 2 model is:-")
print(confusion_in)
print(paste0("The in-sample accuracy of baseline 2 model is: ", sum(diag(confusion_in))/sum(confusion_in)))
```
```{r out-of-sample performance of baseline 2 model, echo=FALSE, message=FALSE, warning=FALSE}
predicted_children_probabilities=baseline2 %>% predict(test_data2,type="response")
predicted_children = ifelse(predicted_children_probabilities>0.5,1,0)
confusion_out=table(y=test_data2$children,yhat=predicted_children)
print("The out-of-sample confusion matrix of baseline 2 model is:-")
print(confusion_out)
baseline2_accuracy=sum(diag(confusion_out))/sum(confusion_out)
print(paste0("The out-of-sample accuracy of baseline 2 model is: ",baseline2_accuracy))
baseline2_precision=confusion_out[4]/(confusion_out[3]+confusion_out[4])
baseline2_recall=confusion_out[4]/(confusion_out[2]+confusion_out[4])
baseline2_f1score=2*baseline2_precision*baseline2_recall/(baseline2_precision+baseline2_recall)
print(paste0("The out-of-sample precision of baseline 2 model is: ",baseline2_precision))
print(paste0("The out-of-sample recall of baseline 2 model is: ",baseline2_recall))
print(paste0("The out-of-sample F1 Score of baseline 2 model is: ",baseline2_f1score))
```
We can see that the baseline 2 model performs better than baseline 1 model.  
```{r baseline 2 v/s baseline 1, echo=FALSE, message=FALSE, warning=FALSE}
print(paste0("Its absolute improvement in terms of out-of-sample accuracy is approximately ", baseline2_accuracy-baseline1_accuracy))
print(paste0("Its relative improvement in terms of out-of-sample accuracy is approximately ", baseline2_accuracy/baseline1_accuracy))
print(paste0("Its absolute improvement in terms of out-of-sample F1 Score is approximately ", baseline2_f1score-baseline1_f1score))
print(paste0("Its relative improvement in terms of out-of-sample F1 Score is approximately ", baseline2_f1score/baseline1_f1score))
```
```{r loading library, echo=FALSE, message=FALSE, warning=FALSE}
library(lubridate)
```
To create the best linear model, we derive a new feature (called as is_holiday) from the arrival_date variable. This feature is a dummy variable that takes the value 1 when the arrival date happens during school summer break (i.e, during May, June, July and August); and Christmas break (i.e, from Dec 25 to Dec 31). It makes sense to introduce this feature as school going children usually make trips with their family during this time.  
```{r creating the feature atleast_2_adults, echo=FALSE, message=FALSE, warning=FALSE}
#hotels_dev['Atleast_2_Adults']=ifelse(hotels_dev$adults>=2,1,0)
```
```{r creating the feature is_holiday, echo=FALSE, message=FALSE, warning=FALSE}
holiday=ifelse((month(hotels_dev$arrival_date)>=5 & month(hotels_dev$arrival_date)<=8) | (month(hotels_dev$arrival_date)==12 & format(as.Date(hotels_dev$arrival_date), format = "%d")>=25),1,0)
hotels_dev['is_holiday']=holiday
```
```{r forming training and test data, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(12345) 
hotels_dev=sample_n(hotels_dev,nrow(hotels_dev))
class_1=hotels_dev[hotels_dev$children==1,]
train_1=class_1[0:3000,]
test_1=class_1[3001:3650,]
class_0=hotels_dev[hotels_dev$children==0,]
train_0=class_0[0:6000,]
test_0=class_0[6001:8000,]
training_data=rbind(train_0,train_1)
set.seed(12345)
training_data=sample_n(training_data,nrow(training_data))
test_data=rbind(test_0,test_1)
set.seed(12345)
test_data=sample_n(test_data,nrow(test_data))
```
```{r creating training and test data for the best linear model, echo=FALSE, message=FALSE, warning=FALSE}
training_data3=subset(training_data,select=-c(arrival_date))
test_data3=subset(test_data,select=-c(arrival_date))
```
```{r introducing best linear model, echo=FALSE, message=FALSE, warning=FALSE}
best_model=glm(children ~ ., data=training_data3,family = 'binomial')
```

```{r in-sample performance of best linear model, echo=FALSE, message=FALSE, warning=FALSE}
predicted_children_probabilities=best_model %>% predict(training_data3,type="response")
predicted_children = ifelse(predicted_children_probabilities>0.5,1,0)
confusion_in=table(y=training_data3$children,yhat=predicted_children)
print("The in-sample confusion matrix of the best linear model is:-")
print(confusion_in)
print(paste0("The in-sample accuracy of the best linear model is: ", sum(diag(confusion_in))/sum(confusion_in)))
```
```{r out-of-sample performance of best linear model, echo=FALSE, message=FALSE, warning=FALSE}
predicted_children_probabilities=best_model %>% predict(test_data3,type="response")
predicted_children = ifelse(predicted_children_probabilities>0.5,1,0)
confusion_out=table(y=test_data3$children,yhat=predicted_children)
print("The out-of-sample confusion matrix of the best linear model is:-")
print(confusion_out)
bestmodel_accuracy=sum(diag(confusion_out))/sum(confusion_out)
print(paste0("The out-of-sample accuracy of the best linear model is: ",bestmodel_accuracy))
bestmodel_precision=confusion_out[4]/(confusion_out[3]+confusion_out[4])
bestmodel_recall=confusion_out[4]/(confusion_out[2]+confusion_out[4])
bestmodel_f1score=2*bestmodel_precision*bestmodel_recall/(bestmodel_precision+bestmodel_recall)
print(paste0("The out-of-sample precision of the best linear model is: ",bestmodel_precision))
print(paste0("The out-of-sample recall of the best linear model is: ",bestmodel_recall))
print(paste0("The out-of-sample F1 Score of the best linear model is: ",bestmodel_f1score))
```
We can see that the best linear model performs better than the baseline 2 model.  
```{r best linear model v/s baseline 1, echo=FALSE, message=FALSE, warning=FALSE}
print(paste0("Its absolute improvement in terms of out-of-sample accuracy is approximately ", bestmodel_accuracy-baseline2_accuracy))
print(paste0("Its relative improvement in terms of out-of-sample accuracy is approximately ", bestmodel_accuracy/baseline2_accuracy))
print(paste0("Its absolute improvement in terms of out-of-sample F1 Score is approximately ", bestmodel_f1score-baseline2_f1score))
print(paste0("Its relative improvement in terms of out-of-sample F1 Score is approximately ", bestmodel_f1score/baseline2_f1score))
```
## Model Validation Step 1: Performance of the Best Linear Model on hotel_val data
```{r Model Validation Step 1, echo=FALSE, message=FALSE, warning=FALSE}
hotels_val = read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/hotels_val.csv")
```
```{r label enconding of hotel_val,  echo=FALSE, message=FALSE, warning=FALSE}
hotels_val$market_segment <- as.numeric(factor(hotels_val$market_segment))
hotels_val$customer_type <- as.numeric(factor(hotels_val$customer_type))
#print(hotels_dev)
hotels_val$hotel <- as.numeric(factor(hotels_val$hotel))
hotels_val$meal <- as.numeric(factor(hotels_val$meal))
hotels_val$distribution_channel <- as.numeric(factor(hotels_val$distribution_channel))
hotels_val$reserved_room_type <- as.numeric(factor(hotels_val$reserved_room_type))
hotels_val$assigned_room_type <- as.numeric(factor(hotels_val$assigned_room_type))
hotels_val$deposit_type <- as.numeric(factor(hotels_val$deposit_type))
hotels_val$required_car_parking_spaces <- as.numeric(factor(hotels_val$required_car_parking_spaces))
```
```{r adding is_holiday feature to hotel_val, echo=FALSE, message=FALSE, warning=FALSE}
holiday=ifelse((month(hotels_val$arrival_date)>=5 & month(hotels_val$arrival_date)<=8) | (month(hotels_val$arrival_date)==12 & format(as.Date(hotels_val$arrival_date), format = "%d")>=25),1,0)
hotels_val['is_holiday']=holiday
```
```{r adding atleast_2_adults feature to hotel_val, echo=FALSE, message=FALSE, warning=FALSE}
#hotels_val['Atleast_2_Adults']=ifelse(hotels_val$adults>=2,1,0)
```
```{r Performance of best linear model on hotel_val data, echo=FALSE, message=FALSE, warning=FALSE}
predicted_children_probabilities=best_model %>% predict(hotels_val,type="response")
predicted_children = ifelse(predicted_children_probabilities>0.5,1,0)
confusion_out=table(y=hotels_val$children,yhat=predicted_children)
print("The confusion matrix of the best linear model is:-")
print(confusion_out)
print(paste0("The accuracy of the best linear model is: ",sum(diag(confusion_out))/sum(confusion_out)))
bestmodel_precision=confusion_out[4]/(confusion_out[3]+confusion_out[4])
bestmodel_recall=confusion_out[4]/(confusion_out[2]+confusion_out[4])
bestmodel_f1score=2*bestmodel_precision*bestmodel_recall/(bestmodel_precision+bestmodel_recall)
print(paste0("The precision of the best linear model is: ",bestmodel_precision))
print(paste0("The recall of the best linear model is: ",bestmodel_recall))
print(paste0("The F1 Score of the best linear model is: ",bestmodel_f1score))
```
We can see that the best linear model gives decent recall but has low precision. That means, it is identifying many false positives.  
```{r loading library pROC, echo=FALSE, message=FALSE, warning=FALSE}
library(pROC)
```
The ROC curve for the best model, using the data in hotels_val is:-    
```{r ROC Curve, echo=FALSE, message=FALSE, warning=FALSE}
roc_score=roc(hotels_val$children, predicted_children)
plot(roc_score ,main ="ROC curve")
```
  
We can see that the ROC curve is towards top-left of the 45-degree diagonal of the ROC space. The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.
```{r installing package, echo=FALSE, message=FALSE, warning=FALSE}
install.packages("caret")
```
```{r loading libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(caret)
library(groupdata2)
```
## Model Validation Step 2
```{r Model Validation Step 2, echo=FALSE, message=FALSE, warning=FALSE}
hotels_val_folds=fold(data=hotels_val,k=20)
```
```{r expected and total booking with children part 1, echo=FALSE, message=FALSE, warning=FALSE}
expected_booking=c()
actual_booking=c()
fold_number=c()
for (i in 1:20){
  fold=hotels_val_folds[hotels_val_folds$.folds==i,]
  predicted_children_probabilities=best_model %>% predict(fold,type="response")
  fold_number=append(fold_number,i)
  expected_booking=append(expected_booking,sum(predicted_children_probabilities))
  actual_booking=append(actual_booking,nrow(fold[fold$children==1,]))
}
```
```{r expected and total booking with children part 2, echo=FALSE, message=FALSE, warning=FALSE}
df = data.frame(fold_number,actual_booking,expected_booking)
df
```
  
We can see that the expected number of bookings with children in a fold is relatively higher than the actual number of bookings with children in that fold. This indicates that there are too many false positives in the model.  
  
Note that the expected number of bookings with children in a fold can become close to the actual number of bookings with children in that fold by adding more data points with zero children in the training data. However, this will increase the false negatives in the data. Thus there exists a trade-off between the two. The optimal problem should be the choice of total number of data points with zero children that should be included in the data such that F1 score is maximized.  