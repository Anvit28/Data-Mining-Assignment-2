---
title: "Data Mining Assignment 2"
author: ""
date: ""
output:
  md_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r including libraries 1, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(modelr)
library(mosaic)
library(rsample)
library(caret)
```
First, we will convert the data into numeric form by doing label encodng.  
```{r label encoding, , echo=FALSE, message=FALSE, warning=FALSE}
SaratogaHouses$heating <- as.numeric(factor(SaratogaHouses$heating))
SaratogaHouses$fuel <- as.numeric(factor(SaratogaHouses$fuel))
SaratogaHouses$sewer <- as.numeric(factor(SaratogaHouses$sewer))
SaratogaHouses$waterfront <- as.numeric(factor(SaratogaHouses$waterfront))
SaratogaHouses$newConstruction <- as.numeric(factor(SaratogaHouses$newConstruction))
SaratogaHouses$centralAir <- as.numeric(factor(SaratogaHouses$centralAir))
```
Next, we split the data into training and test data. The training data comprises of 80% of overall data, and test data comprises of remaining 40% of the data.  
```{r splitting into training and test data,echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)
saratoga_split = initial_split(SaratogaHouses, prop = 0.8, list = FALSE)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)
```
Next, we standardize the data to account for large differences.  
```{r satandardizing the data,echo=FALSE, message=FALSE, warning=FALSE}
saratoga_houses2 <- SaratogaHouses %>% mutate_all(~(scale(.) %>% as.vector))
saratoga_train2 <- saratoga_train %>% mutate_all(~(scale(.) %>% as.vector))
saratoga_test2 <- saratoga_test %>% mutate_all(~(scale(.) %>% as.vector))
```
Now we start with creating the models.To evaluate the models, we compare the out-of-sample mean-squared error.  
We will first check the performance of the "medium" model that we considered in class.  
```{r class model,echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
lm_c = train(
  price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train2,
  method = "lm",
  trControl = trainControl(method = "cv",
              number = 10, verboseIter = TRUE))
```
```{r class model result,echo=FALSE, message=FALSE, warning=FALSE}
print(paste0("The out-of-sample Root Mean-Squared Error of the medium model that we considered in class is: ",rmse(lm_c,saratoga_test2)))
```
*Best Linear Model*  
We create the best linear model by removing the features fireplaces, pctCollege, fuel, sewer, waterfront, centralAir, and landValue. We also create a new feature given by diving bathrooms/rooms; that measures the proportion of bathrooms available per room for the house. Finally since the landValue of the house is a big factor in determing the price, so we scale all these features by landValue.    
```{r my model,echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
lm_me = train(
  price ~(. - fireplaces - pctCollege - fuel - sewer - waterfront - centralAir - landValue + bathrooms/rooms) *(landValue), data=saratoga_train2,
  method = "lm",
  trControl = trainControl(method = "cv",
                           number = 10, verboseIter = TRUE))
```
```{r my model result,echo=FALSE, message=FALSE, warning=FALSE}
print(paste0("The out-of-sample Root Mean-Squared Error of the best linear model is: ",rmse(lm_me,saratoga_test2)))
```
We can see that the out-of-sample Root Mean-Squared Error of the best linear model is lower than the the medium model that we considered in class. Thus it outperforms the medium model that we considered in class.  
*The best K-nearest-neighbor regression model*  
We form the K-nearest-neighbor regression model by including the same features as out best linear model, except that the model that we now use is KNN regression model.  
```{r KNN, echo=FALSE, message=FALSE, warning=FALSE}
K_values <- c(2:200)
RMSE <- c()
for (i in K_values){

#run KNN models
  knn = knnreg(price ~(. - fireplaces - pctCollege - fuel - sewer - waterfront - centralAir - landValue + bathrooms/rooms)*landValue, data= saratoga_train2, k=i)
  #made a vector of RMSE values
  e <- modelr::rmse(knn,saratoga_test2)
  RMSE <- c(RMSE,e)
}


dataframe <- data.frame(K_values, RMSE) 
ggplot(dataframe) +
  geom_line(aes(x=K_values, y=RMSE)) +
  xlab("Values of K") + ylab("RMSE") + 
  ggtitle("Plot of RMSE versus K for trim=350")

#found optimal value of K
minimum <- min(dataframe$RMSE)
K <- K_values[dataframe$RMSE == minimum]
print(paste0("The optimal value of K is: ", K))
print(paste0("The Root Mean Squared Error of the best K-nearest-neighbor regression model for the optimal value of K on test set is: ",minimum))
```
We can see that the out-of-sample Root Mean-Squared Error of the best K-nearest-neighbor regression model is lower than the best linear model. Thus it outperforms the best linear model.  


